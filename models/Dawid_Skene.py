import numpy as np
import math


def is_chance_rate(theta):
    n = theta.shape[0]
    K = theta.shape[1]
    sum = np.sum(theta, axis=0)
    for k in range(K):
        if int(sum[k]) == n:
            return True
    return False


def calculate_num(task_worker_class, n, m, K):
    '''
    :param task_worker_class:   The list of triplets of task, workers and class.
    :param n:                   The number of tasks.
    :paran m:                   The number of workers.
    :param K:                   The number of classes.
    :return num:                Input of DS model. Each element n_{ik}^j is the number of times worker j observes class
                                k from task i.
    '''
    num = np.zeros([n, m, K], dtype=int)
    for i, j, k in task_worker_class:
        num[i, j, k] = 1
    return num


def normalize_each_row(A):
    '''
    :param A: 2-D array
    :return B: Each row of A is normalized. The sum of each row of A is 1.
    '''
    B = np.array([A[i] / np.sum(A[i]) for i in range(A.shape[0])])
    return B


def initialize_T(n, K, num):
    '''
    :param n:   The number of tasks.
    :param K:   The number of classes.
    :param num: Input of DS model.
    :return T:  Initial value of T.
    This function initializes T.
    Each element of T is calculated by base value + random noise.
    The base value is T_{ik} = \sum_{j} num_{ik}^j / \sum_{k} \sum_{j} num_{ik}^j.
    The random noise is generated by normal distribution.
    Then T_{i*} is normalized.
    '''
    # sum_num = np.sum(num, axis=1)
    # sum_sum_num = np.sum(sum_num, axis=1)
    # T = normalize_each_row(np.array([[sum_num[i, k] / sum_sum_num[i] + np.abs(np.random.normal()) for k in range(K)] for i in range(n)]))
    T = np.clip(np.einsum('ki->ik', np.einsum('ijk->ki', num) / np.einsum('ijk->i', num)) + np.random.normal(scale=0.1, size=[n, K]), 0.0, 1.0)
    T = np.einsum('ki->ik', np.einsum('ik->ki', T) / np.sum(T, axis=1))
    return T


def initialize_no_random(n, K, x):
    return np.einsum('ki->ik', np.einsum('ijk->ki', x) / np.einsum('ijk->i', x))


def m_step(T, num, delta=1e-100):
    '''
    :param T:   Each element T_{ik} is the indicator random variable of task i, that is if the true class of this task
                is q then T_{iq} = 1, otherwise T_{ik} = 0.
    :param num: Input of DS model. Each element n_{ik}^j is the number of times worker j observes class k from task i.
    :param delta: Positive small number for making log non-overflow.
    :return pi: New estimate of pi. Each element \pi_{kk'}^j is the probability that worker j will record class k'
                given k is true class.
    :return p:  New estimate of p. Each element p_k is the probability that a task drawn at random is class k.
    This function corresponds to M-step of EM algorithm.
    This function calculates new estimates of pi and p.
    '''
    n = T.shape[0]
    m = num.shape[0]
    K = T.shape[1]
    # sum_T = np.sum(T, axis=0)
    # sum_weighted_T = np.sum(np.array([[[[T[i, k] * num[j, i, k_] + delta for i in range(n)] for k_ in range(K)] for k in range(K)] for j in range(m)]), axis=3)
    # sum_sum_weighted_T = np.sum(sum_weighted_T, axis=2)
    # pi = np.array([[[sum_weighted_T[j, k, k_] / sum_sum_weighted_T[j, k] for k_ in range(K)] for k in range(K)] for j in range(m)])
    # p = (sum_T + delta) / n
    # p = p / np.sum(p)
    p = (np.sum(T, axis=0)) / n
    T += delta
    pi = np.einsum('mjk->jkm', np.einsum('ijm,ik->mjk', num, T) / (np.einsum('ijm,ik->jk', num, T) + delta))
    return pi, p


def e_step(pi, p, num, delta=1e-100):
    '''
    :param pi:  Each element \pi_{kk'}^j is the probability that worker j will record class k' given k is true class.
    :param p:   Each element p_k is the probability that a task drawn at random is class k.
    :param num: Input of DS model. Each element n_{ik}^j is the number of times worker j observes class k from task i.
    :return T:  New estimate of T. Each element T_{ik} is the indicator random variable of task i, that is if the true
                class of this task is q then T_{iq} = 1, otherwise T_{ik} = 0.
    This function corresponds to E-step of EM algorithm.
    This function calculates new estimates of T.
    Actual return values is Pr(T_{ik} = 1 | data).
    So Each element of T is maybe not equal to 1 or 0.
    '''
    #n = num.shape[0]
    #m = pi.shape[0]
    #K = p.shape[0]
    #prob = [[[[(pi[j, k, k_] ** num[i, j, k_]) * p[k] for j in range(m)] for k_ in range(K)] for k in range(K)] for i in range(n)]
    #prod = np.prod(np.prod(prob, axis=3), axis=2)
    #sum_prod = np.sum(prod, axis=1)
    #T = np.array([[prod[i, k] / sum_prod[i] for k in range(K)] for i in range(n)])

    T_ = np.einsum('ik->ki', np.exp(np.einsum('ijm,jkm->ik', num, pi) + np.log(p)))
    T = np.einsum('ki->ik', T_ / np.sum(T_, axis=0))
    return T


def likelihood(pi, p, T, num):
    '''
    :param pi:  Each element \pi_{kk'}^j is the probability that worker j will record class k' given k is true class.
    :param p:   Each element p_k is the probability that a task drawn at random is class k.
    :param T:   Each element T_{ik} is the indicator random variable of task i, that is if the true class of this task
                is q then T_{iq} = 1, otherwise T_{ik} = 0.
    :param num: Input of DS model. Each element n_{ik}^j is the number of times worker j observes class k from task i.
    :return L:  The likelihood given pi, p, T, and num.
    This function calculate the likelihood.
    '''
    n = T.shape[0]
    m = pi.shape[0]
    K = p.shape[0]
    pi_num = np.array([[[[pi[j, k, k_] ** num[j, i, k_] for j in range(m)] for k_ in range(K)] for k in range(K)] for i in range(n)])
    L = np.array([[(p[k] * np.prod(pi_num[i, k])) ** T[i, k] for i in range(n)] for k in range(K)])
    L = np.prod(L)
    if math.isnan(L):
        print(pi)
        print(p)
        print(T)
        print(num)
        raise ValueError
    return L


def log_likelihood(pi, p, T, num):
    '''
    :param pi:  Each element \pi_{kk'}^j is the probability that worker j will record class k' given k is true class.
    :param p:   Each element p_k is the probability that a task drawn at random is class k.
    :param T:   Each element T_{ik} is the indicator random variable of task i, that is if the true class of this task
                is q then T_{iq} = 1, otherwise T_{ik} = 0.
    :param num: Input of DS model. Each element n_{ik}^j is the number of times worker j observes class k from task i.
    :return L:  The log-likelihood given pi, p, T, and num.
    This function calculate the likelihood.
    '''
    n = T.shape[0]
    m = pi.shape[0]
    K = p.shape[0]
    pi_num = np.array([[[[np.log(pi[j, k, k_]) * num[j, i, k_] for j in range(m)] for k_ in range(K)] for k in range(K)] for i in range(n)])
    L = np.array([[(np.log(p[k]) + np.sum(pi_num[i, k])) * T[i, k] for i in range(n)] for k in range(K)])
    L = np.sum(L)
    if math.isnan(L):
        print(pi)
        print(p)
        print(T)
        print(num)
        raise ValueError("Log-Likelihood is Nan.")
    return L


def elbo(x, theta, pi, rho, delta=1e-100):
    l = np.einsum('ijm,ik,jkm->', x, theta, np.log(pi + delta)) + \
        np.einsum('ik,k->', theta, np.log(rho + delta)) + \
        np.einsum('ik,ik->', theta, np.log(theta + delta))
    if np.isnan(l):
        print(x)
        print(theta)
        print(pi)
        print(rho)
        raise ValueError("ELBO is Nan!")
    return l


def convergence_condition(L, L_, epsilon):
    '''
    :param L:               Latest likelihood.
    :param L_:              Previous likelihood.
    :param epsilon:         Accuracy parameter.
    :return true_or_false:  If convergence condition holds then True, otherwise False.
    This function checks convergence condition.
    |L - L_| / L < epsilon.
    '''
    if np.abs(L - L_) / L < epsilon:
        return True
    else:
        return False


def convergence_condition_pi(pi, pi_, epsilon):
    '''
    :param pi:              Latest pi.
    :param pi_:             Previous pi.
    :param epsilon:         Accuracy parameter.
    :return true_or_false: If convergence condition holds then true, otherwise False.
    This function chaecks convergence condition.
    ||pi - pi_|| / ||pi|| < epsilon.
    ||x|| means Frobenius norm.
    In this setting, x is 3-dimensional matrix.
    '''
    true_or_false = True
    m = pi.shape[0]
    for j in range(m):
        if np.linalg.norm(pi[j] - pi_[j]) / np.linalg.norm(pi[j]) >= epsilon:
            true_or_false = False
    return true_or_false


def convergence_condition_elbo(elbo_new, elbo_old, epsilon):
    if elbo_new - elbo_old < 0:
        # print("         ELBO is decrease!!!")
        return False
    elif elbo_new - elbo_old < epsilon:
        return True
    else:
        return False


def EM(num, epsilon, delta, T, cc='l'):
    if cc == 'pi':
        pi, p = m_step(T, num, delta)
        T = e_step(pi, p, num)
        pi_ = pi
        pi, p = m_step(T, num, delta)
        T = e_step(pi, p, num)
        while not(convergence_condition_pi(pi, pi_, epsilon)):
            pi_ = pi
            pi, p = m_step(T, num, delta)
            T = e_step(pi, p, num)
            #print("         ", np.linalg.norm(pi))
        return pi, p, T
    elif cc == 'l':
        pi, p = m_step(T, num, delta)
        T = e_step(pi, p, num)
        L = log_likelihood(pi, p, T, num)
        #print("                 ", L)
        pi, p = m_step(T, num, delta)
        T = e_step(pi, p, num)
        L_ = L
        L = log_likelihood(pi, p, T, num)
        #print("                 ", L)
        while not(convergence_condition(L, L_, epsilon)):
            pi, p = m_step(T, num, delta)
            T = e_step(pi, p, num)
            L_ = L
            L = log_likelihood(pi, p, T, num)
            #print("                 ", L)
        return pi, p, T


def T_is_not_all_same(T):
    n = T.shape[0]
    x = np.sum(T, axis=0)[0]
    if int(n) == int(x) or int(x) == 0:
        return False
    else:
        return True


def DS(task_worker_class, n, m, K, itr):
    '''
    :param task_worker_class:   Input. Each element (i, j, k) represents that worker j labeled task i as class k.
    :param n:                   The number of tasks.
    :param m:                   The number of workers. This value is not used.
    :param K:                   The number of classes.
    :return T:                  Estimates of class of each task. T[i] denotes the class of task i.
    This function estimates the true classes of tasks by using Dawid-Skene algorithm.
    '''
    epsilon = 1e-5
    delta = 1e-30
    num = calculate_num(task_worker_class, n, m, K)
    T = initialize_T(n, K, num)
    pi, p, T = EM(num, epsilon, delta, T)
    L = log_likelihood(pi, p, T, num)
    for i in range(itr):
        #print("     ", i)
        T_ = initialize_T(n, K, num)
        pi_, p_, T_ = EM(num, epsilon, delta, T_)
        L_ = log_likelihood(pi_, p_, T_, num)
        if L_ > L:
            pi, p, T = pi_, p_, T_
            L = L_
    #print(T)
    estimated_T = [np.argmax(T[i]) for i in range(n)]
    return estimated_T


def DS_elbo(task_worker_class, n, m, K, epsilon=1e-2, itr=10):
    x = calculate_num(task_worker_class, n, m, K)
    l__ = -1e+100
    for _ in range(itr):
        theta = initialize_T(n, K, x)
        pi, rho = m_step(theta, x)
        l = elbo(x, theta, pi, rho)
        while True:
            theta = e_step(pi, rho, x)
            pi, rho = m_step(theta, x)
            l_ = elbo(x, theta, pi, rho)
            if convergence_condition_elbo(l_, l, epsilon):
                l = l_
                break
            else:
                l = l_
        if l > l__:
            theta_now = theta
            l__ = l
    g_hat = np.argmax(theta_now, axis=1)
    return g_hat


def inference(task_worker_class, n, m, K, epsilon=1e-2):
    x = calculate_num(task_worker_class, n, m, K)
    c = 1

    theta = initialize_no_random(n, K, x)
    pi, rho = m_step(theta, x)
    l = elbo(x, theta, pi, rho)
    while True:
        theta = e_step(pi, rho, x)
        pi, rho = m_step(theta, x)
        l_ = elbo(x, theta, pi, rho)
        if convergence_condition_elbo(l_, l, epsilon):
            break
        else:
            l = l_

    while is_chance_rate(theta):
        c += 1
        theta = initialize_T(n, K, x)
        pi, rho = m_step(theta, x)
        l = elbo(x, theta, pi, rho)
        while True:
            theta = e_step(pi, rho, x)
            pi, rho = m_step(theta, x)
            l_ = elbo(x, theta, pi, rho)
            if convergence_condition_elbo(l_, l, epsilon):
                break
            else:
                l = l_
        print(c)
        if c >= 100:
            break
    g_hat = np.argmax(theta, axis=1)
    return g_hat, [pi, rho]
